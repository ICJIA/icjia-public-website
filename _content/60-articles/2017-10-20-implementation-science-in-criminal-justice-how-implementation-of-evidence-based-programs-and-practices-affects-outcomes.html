---
title: 'Implementation Science in Criminal Justice: How Implementation of Evidence-based Programs and Practices Affects Outcomes'
_template: article
authors:
  - Lily Gleicher
authorviz: 'false'
area:
  - Evidence-Informed Practices
pubtype:
  - Article
teaser: "With increased attention on the criminal justice system's use of evidence-based practices, focus is needed on the quality of practice implementation and its impact on outcomes. This article defines evidence-based practices, discusses the importance of effective implementation, and outlines the drivers for organizational and operational change."
summary: "With increased attention on the criminal justice system's use of evidence-based practices, focus is needed on the quality of practice implementation and its impact on outcomes. This article defines evidence-based practices, discusses the importance of effective implementation, and outlines the drivers for organizational and operational change."
paragraphType: indent
tabviz: 'false'
tab1: Overview
tab2: Full Report
keywords:
  - implementation
  - evidence-based practices
  - criminal justice
  - organizational readiness
  - program adaptation
splash: /assets/img/splash/iStock-660328504.jpg
leftColumn: |
  <!-- Article Contents links under the side section -->
  <h5 style="color: rgb(68, 68, 68); text-transform: uppercase; padding-bottom: 8px; font-weight: 900; margin-bottom: 10px; margin-left: 10px; border-bottom-color: rgb(204, 204, 204); border-bottom-width: 1px; border-bottom-style: solid;" rel="color: rgb(68, 68, 68); text-transform: uppercase; padding-bottom: 8px; font-weight: 900; margin-bottom: 10px; margin-left: 10px; border-bottom-color: rgb(204, 204, 204); border-bottom-width: 1px; border-bottom-style: solid;">Article Contents</h5>
  <ul class="nav">
  	<li><a class="scrollclass" data-target="#content-wrap">Introduction</a></li>
  	<li><a class="scrollclass" data-target="#what-are-evidence-based-practices">What are Evidence-Based Practices?</a></li>
  	<li><a class="scrollclass" data-target="#implementation-science-and-evidence-based-programs-and-practices">Implementation Science and Evidence-Based Programs and Practices</a></li>
      <li><a class="scrollclass" data-target="#strategic-planning">Strategic Planning</a></li>
  	<li><a class="scrollclass" data-target="#the-challange-of-adapting-an-ebp-to-meet-local-needs-and-capabilities">The Challange of Adapting an EBP to Meet Local Needs and Capabilities</a></li>
          <li><a class="scrollclass" data-target="#conclusion">Conclusion</a></li>
  </ul>
---
<!-- <h1 id="implementation-science-in-criminal-justice-how-implementation-of-evidence-based-programs-and-practices-affects-outcomes">Implementation Science in Criminal Justice: How Implementation of Evidence-based Programs and Practices Affects Outcomes</h1> -->
<h2 id="introduction">Introduction</h2>
<p>Criminal and juvenile justice systems are increasingly training staff in evidence-based practices and programs (EBPs) to enhance public safety.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> EBPs incorporate objective and reliable research
    and data to guide policy and practice decisions, with the aim of improving outcomes.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> State and federal legislatures continue to expand their support for integrating EBPs through the
    provision of funding.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> While EBPs can contribute to increased public safety, reduced recidivism, and increased accountability, they must be implemented with fidelity—adherence to their
    core components with regular training—and make plans to ensure sustainability.</p>
<p>Despite the promise of EBPs, their success is varied, limited by a lack of organizational capacity to effectively implement and sustain them. An evidence-based approach is needed not only on the selection of the EBP, but also on successful implementation
    with both short- and long-term sustainability plans. Implementation science examines how EBPs can be best implemented and how implementation affects immediate and future outcomes.</p>
<p>Policymakers, researchers and practitioners must also focus on maintaining fidelity to the core components of EBPs and how, aside from training, the practicing agency intends to move forward with full implementation and long-term sustainability. When
    translating research into practice, real-world outcomes and benefits are significantly shaped by program quality, organizational development, policy alignment, and quality of implementation, or how the new policy, program, or practice is integrated
    into an organization, funded, executed, and evaluated.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h2 id="what-are-evidence-based-practices">What are Evidence-Based Practices?</h2>
<p>EBPs, while newer to the field of criminal justice and social science in general, have been used in other fields of study. EBPs originated in the medical field, applied to distinguish between effective and ineffective medical practices through scientific
    methods, statistical analysis, research, and patient outcomes.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> Businesses, production and manufacturing sectors, education, public health, mental health, foster care and child well-being
    services, to name a few, have all moved towards utilizing EBPs.</p>
<p>Beginning around the late 1970s, criminal justice researchers sought to develop a systematic process to identify effective and ineffective criminal justice programs and practices; assess study quality of the current literature; and analyze how the current
    literature’s methodological quality supports, or does not support, its outcomes (i.e., the strength and accuracy of outcomes). <sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></p>
<p>Although the terms <em>evidence-based practices</em> and <em>evidence-based programs</em> are frequently used interchangeably, they have slightly different meanings. <em>Evidence-based practices</em> are skills, techniques, strategies, policy initiatives,
    or core intervention components that have accumulated a significant amount of supporting research through high-quality process and outcome evaluations. <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> <em>Evidence-based programs</em>    consist of structured, multi-faceted interventions, comprised of coordinated services or practices, designed to target complex client/consumer problems. <sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> Essentially, evidence-based
    programs help provide the framework for evidence-based practices. For example, Aggression Replacement Training (ART) is an evidence-based program that helps increase prosocial behavior in chronically violent and aggressive youth and adolescents.
    <sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> Evidence-based practices are incorporated within ART’s three components: social skills training, anger control, and moral reasoning. <sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p>To determine whether a program or practice is effective, ideally, evaluations employ a randomized experiment design, which involves the use of random assignment of participants into either the experimental group (in the program) or the control group not
    in the program). A comparison is then made of the impact of that program or practice on similarly situated individuals. <sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup> Although these randomized control trials (RCT) are the “gold
    standard,” they can be difficult to employ in social science; it is important to always abide by ethical and methodological design considerations when employing an RCT design. <sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> When
    an RCT is not an option, a quasi-experimental design can be employed, utilizing a comparison group that was not randomly assigned. <sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> Ultimately, these studies must demonstrate causal
    evidence linking the practice or program with the desired outcomes, while ruling out factors other than the program or practice that may contribute to or influence outcomes or factors that may contribute to differences between groups prior to treatment.
    <sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup></p>
<p>Though there is no specific, agreed upon number of studies that must be reached in order for a program or practice to be considered evidence-based, the Centers for Disease Control and Prevention (CDC) developed a guide to facilitate a common understanding
    of the continuum of evidence of effectiveness, which can help guide practitioners, policymakers, researchers, and other decision-makers in criminal justice. <sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> Another guide, <a href="http://www.preventionresearch.org/StandardsofEvidencebook.pdf"><em>Standards of Evidence</em></a>,
    developed by the Society for Prevention Research—provides criteria for efficacy and dissemination of evidence. <sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> Generally, for something to be deemed evidence-based, the research
    and its outcomes should be rigorous, replicable, generalizable, and objective (<em>Figure 1</em>). <sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup></p>
<h3 align="center" id="figure-1">FIGURE 1</h3>
<h4 align="center" id="defining-evidence-based-practices">Defining Evidence-Based Practices</h4>
<table class="table table-hover table-striped">
    <thead>
        <tr>
            <th style="text-align:center">Overall Effect</th>
            <th style="text-align:center">Requirements</th>
            <th style="text-align:center">Terminology</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="text-align:center">No Effect</td>
            <td style="text-align:center">Little or no evidence exists through the use of reliable, rigorous, replicable, and generalizable research indicating the programs achieve what they are intended to achieve.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup></td>
            <td style="text-align:center">Not Evidence-Informed or Evidence-Based</td>
        </tr>
        <tr>
            <td style="text-align:center">Promising</td>
            <td style="text-align:center">Some evidence exists through the use of reliable, rigorous, replicable, and generalizable research indicating the programs achieve what they are set out to achieve.<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup></td>
            <td style="text-align:center">Evidence-Informed</td>
        </tr>
        <tr>
            <td style="text-align:center">Effective</td>
            <td style="text-align:center">Strong evidence exists through use of reliable, rigorous, replicable, and generalizable research indicating programs achieve what they are set out to achieve.<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup></td>
            <td style="text-align:center">Evidence-Based</td>
        </tr>
    </tbody>
</table>
<hr>
<h5 id="national-resources">National Resources</h5>
<ul>
    <li><a href="https://www.colorado.edu/cspv/blueprints/">Center for the Study and Prevention of Violence</a>, Blueprints for Healthy Youth Development (University of Colorado-Boulder).</li>
    <li><a href="https://www.campbellcollaboration.org/">Campbell Collaboration</a></li>
    <li><a href="https://www.crimesolutions.gov/">Crimesolutions.gov</a></li>
    <li><a href="http://cebcp.org/evidence-based-policing/the-matrix/">Evidence-Based Policing Matrix</a></li>
    <li>Office of Juvenile Justice and Delinquency Prevention’s <a href="https://www.ojjdp.gov/mpg/">Model Programs guide</a>; <a href="http://toptierevidence.org/programs-reviewed/interventions-for-children-age-0-6">Coalition for Evidence-Based Policy</a></li>
    <li>Substance Abuse and Mental Health Services Administration’s (SAMHSA) <a href="http://nrepp.samhsa.gov/AllPrograms.aspx">National Registry of Evidence-Based Programs and Practices</a></li>
</ul>
<hr>
<p>Once an evidence-based program or practice is identified, planning for implementation and sustainability prior to training is fundamental for the success of the organization, its staff, and its clients. It is important to consider that even the most empirically
    sound programs and practices can produce outcomes that are inconsistent, unsustainable, harmful, or generally undesirable when poorly implemented.<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> Research supports the necessity
    for fidelity and high quality implementation in order to most successfully implement and sustain EBPs.<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></p>
<h2 id="implementation-science-and-evidence-based-programs-and-practices">Implementation Science and Evidence-Based Programs and Practices</h2>
<p>Before implementing an EBP, it is important to gauge an organization’s development and capacity to implement a new program.<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> This helps provide insight on organizational culture, including
    shared values or beliefs that govern staff within an organization, climate or how staff experience organizational culture; leadership; communication and decision-making within an organization; alignment of policies and practices with the potential
    adoption of a new EBP; alignment of policies and practices with the mission of the organization; and goals and strategic plan.<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></p>
<h3 id="measuring-readiness-how-ready-is-your-organization-to-make-a-change">Measuring Readiness: How Ready is your Organization to Make a Change?</h3>
<p>Assessment is one way to measure an organization’s readiness for change, which can impact the implementation of EBPs and their success.<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup> Readiness includes preparing staff at every level
    for the implementation and sustainability of the program or practice, as well as aligning policies and practices to support the staff and the organization in using the EBP. <sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup> Organizational
    readiness is an imperative precursor to successful implementation and sustainability—without it, change is more difficult to make and may ultimately result in a failed program.<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup> Successfully
    implementing change can be difficult, however, with many organizational barriers impeding the process.<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup></p>
<p>Several factors are associated with readiness for change, including motivational readiness, institutional resources, staff attributes, and organizational climate (<em>Figure 2</em>). Lower levels of staff cynicism toward change, favorable perceptions
    of leadership, a supportive environment within the organization, and an increase in interagency networks can influence an organization’s readiness for change.<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup></p>
    <br>
<h3 align="center" id="figure-2">FIGURE 2</h3>
<h4 align="center" id="organizational-readiness-factors">Organizational Readiness Factors</h4>

<p align="center"><img class="img-responsive" alt="”figure" 2”="" src="/assets/img/articles/Figure-2-Org-Readiness-factors.PNG"></p>

<h6 id="source-texas-christian-universitys-organizational-readiness-for-change-tool-lehman-w-e-k-greener-j-m-simpson-d-d-2002-assessing-organizational-readiness-for-change-journal-of-substance-abuse-treatment-22-197-209"><em>Source</em>: Texas Christian University’s Organizational Readiness for Change Tool; Lehman, W. E. K., Greener, J. M., & Simpson, D. D. (2002). Assessing organizational readiness for change. Journal of Substance Abuse Treatment, 22, 197-209.</h6>
<br>
<p>Organizations can measure their readiness for change by using one of the following tools:</p>
<ul>
    <li><a href="http://implementation.fpg.unc.edu/sites/implementation.fpg.unc.edu/files/AIModules-Activity-5-1-GettingReadyForChange.pdf">Getting "Ready for Change</a> National Implementation Research Network (NIRN) Active Implementation Hub activity.</li>
    <li><a href="https://ibr.tcu.edu/forms/organizational-staff-assessments/">Texas Christian University’s</a> (TCU) Organizational Readiness for Change (TCU ORC), TCU Survey of Structure and Operations (TCU-SSO), TCU Survey of Organizational Functioning
        (TCU SOF), and TCU Survey of Transformational Leadership (TCU-STL S).</li>
    <li><a href="http://www.nccmt.ca/knowledge-repositories/search/34">Evidence-based Practice Attitude Scale</a><sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup></li>
    <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3904699/">Organizational Readiness for Implementing Change (ORIC)</a></li>
    <li><a href="http://www.cebc4cw.org/implementing-programs/tools/measures/">Dimensions of Organizational Readiness-Revised (DOOR-R)</a></li>
</ul>
<h4 id="are-your-staff-sufficiently-informed-of-the-change">Are Your Staff Sufficiently Informed of the Change?</h4>
<p>In order for organizations to prepare for implementation, all staff should have a clear understanding of</p>
<ul>
    <li>Who is in charge of managing the implementation and the EBP itself.</li>
    <li>What the program or practice is —requiring the education staff and stakeholders with regard to the why and how of the program or practice.</li>
    <li>The essential functions of the program or practice and how those functions are operationalized.</li>
    <li>What the strategic plan looks like for short- and long-term implementation and the sustainability of the program or practice.</li>
    <li>To the process for communicating questions, comments, or concerns about the program or practice.</li>
    <li>The quality assurance process—or tracking of program process and outcomes on a continual basis—that assesses fidelity to the program or practice. This also helps identify any obstacles or barriers, areas that may need modification, and any gaps in
        information or services.<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup></li>
</ul>
<h4 id="does-your-organization-have-an-implementation-team">Does your Organization have an Implementation team?</h4>
<p>A cross-section of staff and stakeholders should comprise an organization’s implementation team to most successfully implement and sustain an EBP.<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup> The implementation team can support
    implementation, sustainability, and the process of scaling-up (i.e., increasing in capacity or use of) the EBP.<sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup> Stakeholders and all organization staff should understand that implementation
    and sustainability is an ongoing, multi-stage process that often includes barriers and resistance to change. Further, support from organization leaders and upper-level staff and management, including involvement in the training process and experience
    or practice with line staff in the use of the program or practice, helps support broader acceptance of organizational change.<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup> In particular, the implementation process can be significantly
    less stressful to all levels of staff when implementation efforts focus on the individual staff, agency, and system levels of implementation, making the process more inclusive and transparent from the bottom-up and the top-down.<sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup>    One way to do this is by using NIRN’s <a href="http://implementation.fpg.unc.edu/sites/implementation.fpg.unc.edu/files/NIRN-Education-TheHexagonDiscussionCaptureTool.pdf">Hexagon Tool </a><sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup>,
    which helps organizations appropriately select and assess elements of a program or practice.</p>
<h4 id="what-are-your-organizations-strengths-and-weaknesses">What are your Organization’s Strengths and Weaknesses?</h4>
<p>In addition, an organization should examine its current capacity, assessing both strengths and weaknesses to understand the organization’s landscape for implementation. One way to do this is through a <a href="http://www.ncjp.org/strategic-planning/overview/additional-planning-tools/swot-analysis">SWOT analysis</a>—a
    group process to compile and analyze the organizations strengths, weaknesses, opportunities, and threats.<sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup> Such analysis can help inform strategic planning by identifying potential
    strategies for implementation.</p>
<p>During the planning process, an organization should think “big picture” on the program or practice goal to account for how change may impact the organization as a whole. Implementing change may require a holistic look at organizational policies, regulations,
    guidelines, procedures, and practices, and how they may align with the new change.<sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup> In particular, organizations should consider how:</p>
<ul>
    <li>The EBP connects to other parts of the organization as well as to organizational and systemic goals.</li>
    <li>Current policies (organizational, local, county, state, federal) may support or conflict with the EBP—consider new policies to support EBPs while discontinuing ineffective or conflicting policies in order to align policy and practice.</li>
    <li>Communication will occur within the organization and external to the organization, promoting honest and open discussion.</li>
    <li>The use of data can drive decision-making. This allows for continual assessments of the organization and its goals, policies, practices, and outcomes.<sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup></li>
</ul>
<h2 id="strategic-planning">Strategic Planning</h2>
<h3 id="assessing-implementation-drivers-for-change">Assessing Implementation Drivers for Change</h3>
<p>In addition to organizational readiness assessments, strategic planning requires assessing key “drivers” for organizational change prior to implementation and on an ongoing basis.</p>
<p>Organizations should assess the components that drive change within an organization, or drivers. Drivers are categorized as <em>competency drivers,</em> organization drivers, and leadership drivers (<em>Figure 3</em>).<sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup>    These implementation drivers support organizational capacity for creating “program, practice, and systems level changes needed to achieve improved population outcomes.”<sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup> These drivers
    are compensatory and integrated in nature, working together to enhance quality, fidelity, and consistency in program implementation to improve outcomes.<sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup></p>
<br>
<h3 align="center" id="figure-3">FIGURE 3</h3>
<h4 align="center" id="implementation-drivers-of-organizational-change">Implementation Drivers of Organizational Change</h4>

<p align="center"><img class="img-responsive" alt="”figure" 3”="" src="/assets/img/articles/Figure-3-implementation-drivers.PNG"></p>

<h6 id="figure-source-bertram-r-m-blase-k-a-fixsen-d-l-2015-improving-programs-and-outcomes-implementation-frameworks-and-organization-change-research-on-social-work-practice-254-477-487"><em>Figure Source</em>: Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. Research on Social Work Practice, 25(4), 477-487.</h6>
<br>
<h4 id="competency-drivers">Competency Drivers</h4>
<p>Competency drivers assist in the development and enhancement of competency and confidence among staff.<sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup> These drivers are vital for selecting, teaching, understanding, supporting, and
    assessing the use of the EBP.</p>
<p><strong>Staff selection</strong> refers to hiring qualified staff at all levels within the organization to train, coach, assess, and execute EBPs. Research suggests that individuals who are open to learning, obtaining, and honing in on new or previously
    learned skills may be more willing to learn and integrate new ways of working within their current responsibilities.<sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup> Further, findings from an evaluation of staff quality and program
    effectiveness of 64 community-based correctional facilities and halfway houses in Ohio indicated that 28 percent of program variation was explained by staff characteristics, training, and supervision; positive staff characteristics and training were
    associated with substantial recidivism reduction.<sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup></p>
<p><strong>Training</strong>, both pre-service (beginning) and in-service (continued), is the impetus for staff behavioral change in implementing and assessing a new EBP. Training is the first step in assisting the transfer of knowledge from research to
    practice. Training should incorporate information on EBP theory, philosophy, values, and rationales of key components and for the organization’s adoption of the EBP, and the ability to listen, watch, practice, and receive feedback on new skills.
    <sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup>
</p>
<p><strong>Ongoing coaching and consultation</strong> integrates newly learned concepts into practice. This can greatly increase staff competency and confidence in using the new program or practice.<sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup>    For example, in a meta-analysis of 21 studies on motivational interviewing, skills eroded for staff who were not provided coaching and feedback post-training compared to those who were.<sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup>    An estimated 10 percent of information is ultimately transferred from training to practice.<sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup> Further, in a study of probation and parole officers trained in an evidence-based supervision
    model, officers who were trained and coached monthly engaged in more proficient use of practices than those who were not coached after the initial training.<sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup></p>
<p><strong>Staff performance assessments</strong> can evaluate staff use of and fidelity to the EBP, as well as outcomes related to those processes.<sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> Staff performance assessments should
    incorporate staff use of, competency in, and fidelity to program or practice components.<sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup> This helps practitioners gain better understanding of their strengths and areas for improvement
    and organizations gain better understanding of implementation progress and efforts.<sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup></p>
<h4 id="organization-drivers">Organization Drivers</h4>
<p>Organization drivers help create supportive environments that increase the accessibility and efficacy of staff selection, training, coaching, and performance via a supportive and welcoming administration, as well as access to funding and resources.
    <sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup> In a study of EBPs in substance use treatment, researchers found that a supportive environment for new programs and practices, training, resources, and interagency networks was related to increased use of
        EBPs.<sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup></p>
<p><strong>Decision support data systems</strong> are central in identifying and assessing key aspects of organizational performance, such as fidelity, outcomes, and quality improvement information, to support the continued improvement and efficacy of EBP
    implementation and sustainability.<sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup> This includes <a href="https://pdfs.semanticscholar.org/6f7c/e3448572672c4c086a44fc9fd360e8817367.pdf">continuous quality improvement or CQI</a>    <sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup> as a way to monitor organizational processes and outcomes in order to ensure that the EBPs are delivered as intended. CQI ultimately helps organizations improve performance of current
    and/or new practices.<sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup> In addition, a quality assurance process is instrumental, as the audit helps to best identify and rectify staff deviation from EBP policy or protocol.<sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup></p>
<p><strong>Facilitative administrative supports</strong> are an organization’s policies, protocols, structures, culture, and climate that act to enhance, support, and facilitate organizational changes to align with staff needs.<sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup>    To do this, leadership provides support to the organization as a whole, making use of organizational data to inform decision-making, keep staff on track, and remain focused on the successful development of skills in order to implement and successfully
    use a new EBP (or enhance other attempted EBP previously implemented) EBP.<sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup></p>
<p><strong>Systems intervention</strong> are strategies to help analyze system-level factors that support outcomes, such as the ever-changing context of federal, state, organizational, funding resources and availability, human resources, and community-level
    policies and practices.<sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup></p>
<h4 id="leadership-drivers">Leadership Drivers</h4>
<p>Implementation of any EBP requires effective leadership to support the staff as well as the organization as a whole in obtaining desired outcomes. Leadership drivers help identify appropriate leadership skills, capabilities, and strategies in order to
    institute, repurpose, modify/adjust, and monitor both competency and organization drivers throughout implementation and further sustainability.<sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup></p>
<p><strong>Technical leadership</strong> is most common with traditional management styles that identify, clearly and precisely, strategies, problems, and solutions.<sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup> Technical leadership
    is most synonymous with effective management. It primarily deals with problems for which there is an agreed upon understanding of the nature of the problem and how to resolve it.<sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup></p>
<p><strong>Adaptive leadership</strong> tends to occur when the problems arise from more complex organizational conditions in which organizational actors are not all in agreement with the problem and solution. Generally, adaptive leadership styles incorporate
    the use of a group of people who work collaboratively to identify the problem and possible solutions.<sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup> The importance of leadership drivers is knowing when each type of leadership
    is most appropriate.<sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup></p>
<p>The use of implementation frameworks and implementation drivers throughout the process of implementation and future sustainability is vital to the quality and effective delivery of an EBP.<sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup>    These implementation drivers are integrated in nature; a change or adjustment in one implementation driver might necessitate change in other implementation drivers.<sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup> These drivers
    also are compensatory in nature; where there is less of one driver, another may be used to supplement. However, this should be done only after careful consideration of each implementation driver.<sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup></p>
<h2 id="the-challange-of-adapting-an-ebp-to-meet-local-needs-and-capabilities">The Challange of Adapting an EBP to Meet Local Needs and Capabilities</h2>
<p>One challenge in implementing EBPs is knowing when or how to adapt the EBP to reflect local differences. While an EBP is the ideal, not every local, county, or state operates the same; this may result in potential barriers to implementing an EBP with
    fidelity, for example, for differing local policies, capacity, target population, and staff skill level.<sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup> Adaptations may be necessary, but it is recommended to first implement with
    fidelity to the original EBP. Research indicates that adaptations made after implementation were more successful than those made prior to implementation.<sup class="footnote-ref"><a href="#fn72" id="fnref72">[72]</a></sup> Further, adaptations must
    be structured around the essential functions (or core components) of the EBP in order to prevent compromising the effectiveness of the program or practice.<sup class="footnote-ref"><a href="#fn73" id="fnref73">[73]</a></sup> Adaptations can be done
    with high or low fidelity, aligning or drifting from the essential functions of the EBP.<sup class="footnote-ref"><a href="#fn74" id="fnref74">[74]</a></sup> Further, adaptation may be appropriate only up to a certain point; too much and the changes
    may result in program “drift,” transforming the EBP into something that is not an EBP.<sup class="footnote-ref"><a href="#fn75" id="fnref75">[75]</a></sup></p>
<p>In a 2013 study of the sustainability of evidence-based school, community/family-focused, and family treatment programs, researchers noted that sustainability suffered when many changes were made to fit the program to the population, setting, and needs.
    <sup class="footnote-ref"><a href="#fn76" id="fnref76">[76]</a></sup> Most frequently, programs identified adaptations to program procedures, dose, and content.<sup class="footnote-ref"><a href="#fn77" id="fnref77">[77]</a></sup> The top five reasons for these adaptations
        were:</p>
<ul>
    <li>Lack of time (80 percent).</li>
    <li>Limited resources (72 percent).</li>
    <li>Participant retention difficulty (71 percent).</li>
    <li>Resistance from implementers (64 percent).</li>
    <li>Difficulty recruiting participants (61 percent).<sup class="footnote-ref"><a href="#fn78" id="fnref78">[78]</a></sup></li>
</ul>
<p>Most adaptations were reactive in nature, related to logistical fit (i.e., compatibility issues related to implementers’ or target populations’ capacity—resources, time, skills, knowledge, and schedules),<sup class="footnote-ref"><a href="#fn79" id="fnref79">[79]</a></sup>    and were negatively aligned with—or had drifted from—the program goals and theory.<sup class="footnote-ref"><a href="#fn80" id="fnref80">[80]</a></sup> Another study of school-based substance use prevention programs found that, on average, teachers
    had more than three adaptations to the program—63 percent of which were negatively adapted (i.e., drifted from the original program theory and goals), increasing the likelihood of null or harmful effects on the youth served.<sup class="footnote-ref"><a href="#fn81" id="fnref81">[81]</a></sup></p>
<p>In a study of an evidence-based adolescent and family programs in several sites, a majority of program modifications were related to philosophical issues—misalignment of organizational or participant values or philosophy (58 percent) —while the deletion
    of program components was most likely due to logistical issues—misalignment of organizational and program capacity or context (77 percent).<sup class="footnote-ref"><a href="#fn82" id="fnref82">[82]</a></sup> Forty-two percent of additions and changes
    to the program were as likely to be aligned with the program’s theory or logic model ( positively aligned) as they were to be misaligned with the program’s theory or logic model ( negatively aligned), whereas deletion of program components and functions
    was most likely negatively aligned with the program’s theory and goals (82 percent).<sup class="footnote-ref"><a href="#fn83" id="fnref83">[83]</a></sup> This suggests that program modifications involving the removal of parts of the program are more
    likely to misalign with the original program theory and logic model—or the components that are most effective within a program. This research suggests that adaptation to EBPs should be done in an objective manner, based on technical, theoretical,
    and rigorous evidence of such adaptations rather than subjective stances or individual beliefs.<sup class="footnote-ref"><a href="#fn84" id="fnref84">[84]</a></sup></p>
<h2 id="conclusion">Conclusion</h2>
<p>Implementation is a complex, continuous process; EBPs should be continually monitored and evaluated for efficacy and fidelity as they relate to process and outcomes. Adopting a comprehensive, multifaceted program or practice within an organizational setting
    must be strategically translated into a complex, ever-changing system, interplaying between EBP characteristics, providers, and organizational and service delivery settings.<sup class="footnote-ref"><a href="#fn85" id="fnref85">[85]</a></sup> Because
    criminal justice organizations are dynamic, influenced by internal and external political, social, moral, and economic pressures, it is important for organizations to build an internal capacity to deliver the program or practice and adapt or modify
    with the ebb and flow of the organization. This upfront planning can greatly improve not only the outcomes of the EBP, but also how staff receive these changes and ultimately use them in their day-to-day activities.</p>
<p><em>This project was supported by Award No. 13-DJ-BX-0012 awarded by the Bureau of Justice Assistance, Department of Justice. The opinions, findings, and conclusions or recommendations expressed in this publication are those of the author and do not necessarily reflect the views of the Department of Justice or the Illinois Criminal Justice Information Authority.</em></p>
<h2 id="references">References</h2>
<hr class="footnotes-sep">
<section class="footnotes">
    <ol class="footnotes-list">
        <li id="fn1" class="footnote-item">
            Lipsey, M. W. (2010). <em>Improving the effectiveness of juvenile justice programs: A new perspective on evidence-based practice.</em> Washington, DC: Georgetown University, Center for Juvenile Justice Reform.; Przybylski, R. (2012). Introduction:
                Special issue on evidence-based policy and practice. <em>Justice Research and Policy</em>, 14(1), 1–15. <a href="#fnref1" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn2" class="footnote-item">
            Lum, C., Koper, C., & Telep, C. (2015). <em>Evidence-based policing matrix.</em> Fairfax, VA: George Mason Univeristy, Center for Evidence-Based Crime Policy.; Lum, C., Koper, C. S., & Telep, C. W. (2011). The evidence-based policing
                matrix. <em>Journal of Experimental Criminology</em>, 7, 3-26.; Lum, C., & Koper, C. S. (2015). <em>Evidence-based policing</em>. In R. Dunham and G. Alpert (Eds.) Critical Issues in Policing (7th ed.). Long Grove, IL: Waveland Press.
                <a href="#fnref2" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn3" class="footnote-item">
            Taxman, F. S., Pattavina, A., & Caudy, M. (2014). Justice reinvestment in the United States: An empirical assessment of the potential impact of increased correctional programming and recidivism.* Victims & Offenders: An International
                Journal of Evidence-Based Research, Policy, and Practice*, 9(1), 50-75. <a href="#fnref3" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn4" class="footnote-item">
            Fixsen, D. L., Naoom, S. F., Blase, K. A., Friedman, R. M. & Wallace, F. (2005). <em>Implementation research: A synthesis of the literature</em>. Tampa, FL: University of South Florida, Louis de la Parte Florida Mental Health Institute,
                The National Implementation Research Network (FMHI Publication #231). <a href="#fnref4" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn5" class="footnote-item">
            Williams-Taylor, L. (2007). <em>Evidence-based programs and practices: What does it all mean?</em> Boynton Beach, FL: Research Review, Children’s Services Council of Palm Beach County. Retrieved from <a href="http://bit.ly/2gQW1Yq">http://bit.ly/2gQW1Yq</a>                ; Przybylski, R. & Orchowsky, S. (2015). <em>EBPs.</em> Washington, DC: Justice Research and Statistics Association. Retrieved from <a href="http://bit.ly/2todZmB">http://bit.ly/2todZmB</a>. <a href="#fnref5" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn6" class="footnote-item">
            Williams-Taylor, L. (2007). <em>Evidence-based programs and practices: What does it all mean?</em> Boynton Beach, FL: Research Review, Children’s Services Council of Palm Beach County. Retrieved from <a href="http://bit.ly/2gQW1Yq">http://bit.ly/2gQW1Yq</a>                ; Przybylski, R. & Orchowsky, S. (2015). <em>EBPs</em>. Washington, DC: Justice Research and Statistics Association. Retrieved from <a href="http://bit.ly/2todZmB">http://bit.ly/2todZmB</a>. <a href="#fnref6" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn7" class="footnote-item">
            Andrews, D. A., & Bonta, J. (2012). <em>The psychology of criminal conduct (5th ed.).</em> Newark, NJ: LexisNexis.; EPISCenter. (N.d.) <em>Defining evidence based programs</em>. University Park, PA: Author. Retrieved from <a href="http://www.episcenter.psu.edu/ebp/definition">http://www.episcenter.psu.edu/ebp/definition</a>.
                <a href="#fnref7" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn8" class="footnote-item">
            Andrews, D. A., & Bonta, J. (2012). <em>The psychology of criminal conduct (5th ed.).</em> Newark, NJ: LexisNexis.; EPISCenter. (N.d.) <em>Defining evidence based programs</em>. University Park, PA: Author. Retrieved from <a href="http://bit.ly/2kWLRaF.;">http://bit.ly/2kWLRaF.;</a>                Williams-Taylor, L. (2007). <em>Evidence-based programs and practices: What does it all mean?</em> Boynton Beach, FL: Research Review, Children’s Services Council of Palm Beach County. Retrieved from <a href="http://bit.ly/2gQW1Yq">http://bit.ly/2gQW1Yq</a>                ; Przybylski, R. & Orchowsky, S. (2015). <em>EBPs</em>. Washington, DC: Justice Research and Statistics Association. Retrieved from <a href="http://bit.ly/2todZmB">http://bit.ly/2todZmB</a>. <a href="#fnref8" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn9" class="footnote-item">
            Glick, B. (1996). Aggression Replacement Training in Children and Adolescents. <em>Hatherleigh Guide to Child and Adolescent Therapy,</em> 5, 191–226. <a href="#fnref9" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn10" class="footnote-item">
            Glick, B. (1996). Aggression Replacement Training in Children and Adolescents. <em>Hatherleigh Guide to Child and Adolescent Therapy,</em> 5, 191–226. <a href="#fnref10" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn11" class="footnote-item">
            Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). <em>Experimental and quasi-experimental design for generalized causal inference.</em> Belmont, CA: Wadsworth, Cengage Learning. <a href="#fnref11" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn12" class="footnote-item">
            This includes how individuals are chosen—or not chosen—to participate in the experimental group, particularly because many of the programs are also intertwined with legal and procedural decisions; when randomization should be carried out based;
                and lack of popularity among criminal justice agencies in employing RCT designs. See Asscher, J. J., Dekovic, M., van der Laan, P. H., Prins, P. J. M., & van Arum, S. (2007). Implementing randomized experiments in criminal justice
                settings: An evaluation of multisystemic therapy in the Netherlands. <em>Journal of Experimental Criminology,</em> 3(2), 113-129. <a href="#fnref12" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn13" class="footnote-item">
            Trochim, W. M. K., & Donnelly, J. P. (2007). <em>Research methods knowledge base.</em> Mason, OH: Thompson Custom Publishing. <a href="#fnref13" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn14" class="footnote-item">
            Gendreau, P. (1996). <em>The principles of effective intervention with offenders.</em> In A. T. Harland (Ed.), Choosing correctional interventions that work: Defining the demand and evaluating the supply (pp. 117-130). Newbury Park, CA: Sage.;
                Andrews, D. A., & Bonta, J. (2012). <em>The psychology of criminal conduct (5th ed.).</em> Newark, NJ: LexisNexis.; Orchowsky, S. (2014). <em>An introduction to evidence-based practices</em>. Justice Research and Statistics Association.
                <a href="#fnref14" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn15" class="footnote-item">
            Puddy, R. W., & Wilkins, N. (2011). <em>Understanding evidence part 1: Best available research evidence. A guide to the continuum of evidence of effectiveness.</em> Atlanta, GA: Centers for Disease Control and Prevention. <a href="#fnref15" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn16" class="footnote-item">
            Flay, B. R., Biglan, A., Boruch, R. F., Castro, F. G., Gottfredson, D., Kellam, S., . . . Ji, P. (2005). Standards of evidence: Criteria for efficacy, effectiveness and dissemination. <em>Prevention Science,</em> 6(3), 151-175. <a href="http://bit.ly/2yyWrtw">http://bit.ly/2yyWrtw</a>.
                <a href="#fnref16" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn17" class="footnote-item">
            Trochim, W. M. K., & Donnelly, J. P. (2007). <em>Research methods knowledge base.</em> Mason, OH: Thompson Custom Publishing. <a href="#fnref17" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn18" class="footnote-item">
            <a href="http://Crimesolutions.gov">Crimesolutions.gov</a>. (2011). <em>Glossary.</em> Washington, DC: National Institute of Justice, Office of Justice Programs. Washington, DC Retrieved from <a href="http://bit.ly/2oieUTg.;">http://bit.ly/2oieUTg.;</a>                Blueprints Program Model. (2012-2016). <em>Program criteria.</em> Boulder, CO: Blueprints for Healthy Youth Development. Retrieved from <a href="http://bit.ly/2yqORS3">http://bit.ly/2yqORS3</a>. <a href="#fnref18" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn19" class="footnote-item">
            <a href="http://Crimesolutions.gov">Crimesolutions.gov</a>. (2011). <em>Glossary.</em> Washington, DC: National Institute of Justice, Office of Justice Programs. Washington, DC Retrieved from <a href="http://bit.ly/2oieUTg.;">http://bit.ly/2oieUTg.;</a>                Blueprints Program Model. (2012-2016). <em>Program criteria.</em> Boulder, CO: Blueprints for Healthy Youth Development. Retrieved from <a href="http://bit.ly/2yqORS3">http://bit.ly/2yqORS3</a>. <a href="#fnref19" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn20" class="footnote-item">
            <a href="http://Crimesolutions.gov">Crimesolutions.gov</a>. (2011). <em>Glossary.</em> Washington, DC: National Institute of Justice, Office of Justice Programs. Washington, DC Retrieved from <a href="http://bit.ly/2oieUTg.;">http://bit.ly/2oieUTg.;</a>                Blueprints Program Model. (2012-2016). <em>Program criteria.</em> Boulder, CO: Blueprints for Healthy Youth Development. Retrieved from <a href="http://bit.ly/2yqORS3">http://bit.ly/2yqORS3</a>. <a href="#fnref20" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn21" class="footnote-item">
            Fixsen, D. L., Naoom, S. F., Blase, K. A., Friedman, R. M. & Wallace, F. (2005). <em>Implementation research: A synthesis of the literature</em>. Tampa, FL: University of South Florida, Louis de la Parte Florida Mental Health Institute,
                The National Implementation Research Network (FMHI Publication #231). <a href="#fnref21" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn22" class="footnote-item">
            Lowenkamp, C. T., Flores, A. W., Holsinger, A. M., Makarios, M. D., & Latessa, E. J. (2010). Intensive supervision programs: Does program philosophy and the principles of effective intervention matter? <em>Journal of Criminal Justice,</em>                38, 368-375. Schweitzer, M., Kishimoto, E., Latessa, E. J., & Rogalski-Davis, L. (2015). Implementing an evidence-based program model: A real world approach to effective correctional treatment. <em>Offender Programs Report,</em> 19(3),
                33-48. <a href="#fnref22" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn23" class="footnote-item">
            Among other sources, the <a href="http://nirn.fpg.unc.edu/about-nirn/our-approach">National Implementation Research Network (NIRN)</a> out of the University of North Carolina at Chapel Hill’s FPG Child Development Institute, provides important
                resources regarding implementation science and how to most effectively implement and sustain EBPs in a variety of fields, including criminal justice. Through NIRN’s amalgamation and consolidation of research and literature, they created
                many quick planning tools in addition to an <a href="http://implementation.fpg.unc.edu/resources/results/taxonomy%3A23?o=nirn">Active Implementation Hub</a> that provides resources including cases and examples; research, publications,
                presentations; and learning modules. Organizations can use the concepts and frameworks as they consider, and later implement, EBPs within their organization. <a href="#fnref23" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn24" class="footnote-item">
            Crime and Justice Institute and Guevara, M., Loeffler-Cobia, J., Rhyne, C., & Sachwald, J. (2011). <em>Putting the pieced together: Practical strategies for implementing evidence-based practices.</em> Washington, DC: National Institute
                of Corrections. <a href="#fnref24" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn25" class="footnote-item">
            Lerch, J., Viglione, J., Eley, E., James-Andrews, S., & Taxman, F. (2011). Organizational readiness in corrections. <em>Federal Probation</em>, 75(5), 5-10. <a href="#fnref25" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn26" class="footnote-item">
            Weiner, B. J. (2009). A theory of organizational readiness for change. <em>Implementation Science,</em> 4(67), 1-9. <a href="#fnref26" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn27" class="footnote-item">
            Weiner, B. J. (2009). A theory of organizational readiness for change. <em>Implementation Science,</em> 4(67), 1-9. <a href="#fnref27" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn28" class="footnote-item">
            Taxman, F. S., Henderson, C., Young, D., & Farrell, J. (2014). The impact of training interventions on organizational readiness to support innovations in juvenile justice offices. <em>Administration and Policy in Mental Health</em>, 41(2),
                177-188. <a href="#fnref28" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn29" class="footnote-item">
            Farrell, J. L., Young, D. W., & Taxman, F. S. (2011). Effects of organizational factors on use of juvenile supervision practices. <em>Criminal Justice and Behavior</em>, 38(6), 565-583.; Henderson, C. E., Taxman, F. S., & Young, D.
                W. (2007). A Rasch model analysis of evidence-based treatment practices used in the criminal justice system. <em>Drug and Alcohol Dependence</em>, 93, 163-175. <a href="#fnref29" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn30" class="footnote-item">
            Aarons, G. A., Ehrhart, M. G., Torres, E. M., Finn, N. K., & Roesch, S. C. (2016). Validation of the implementation leadership scale (ILS) in substance use disorder treatment organizations. <em>Journal of Substance Abuse Treatment,</em>                68, 31-35. <a href="#fnref30" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn31" class="footnote-item">
            Van Dyke, M., Blasé, K., Sims, B., & Fixsen, D. (2013).<em>Implementation drivers: Team review and planning.</em> Chapel Hill, NC: University of North Carolina Chapel Hill, National Implementation Research Network (NIRN), Frank Porter
                Graham Child Development Institute. <a href="#fnref31" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn32" class="footnote-item">
            Gotham, H. J., White, M. K., Bergethon, H. S., Feeney, T., Cho, D. W., & Keehn, B. (2008). An implementation story: Moving the GAIN from pilot project to statewide use. <em>Journal of Psychoactive Drugs</em>, 40(1), 97. <a href="#fnref32" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn33" class="footnote-item">
            The National Implementation Research Networks’s Active Implementation Hub. (2013-2015). Retrieved from <a href="http://unc.live/2x5sbmy">http://unc.live/2x5sbmy</a> <a href="#fnref33" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn34" class="footnote-item">
            Gotham, H. J., White, M. K., Bergethon, H. S., Feeney, T., Cho, D. W., & Keehn, B. (2008). An implementation story: Moving the GAIN from pilot project to statewide use. <em>Journal of Psychoactive Drugs</em>, 40(1), 97. <a href="#fnref34" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn35" class="footnote-item">
            Gotham, H. J., White, M. K., Bergethon, H. S., Feeney, T., Cho, D. W., & Keehn, B. (2008). An implementation story: Moving the GAIN from pilot project to statewide use. <em>Journal of Psychoactive Drugs</em>, 40(1), 97. <a href="#fnref35" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn36" class="footnote-item">
            While the tool is framed for use in schools, it is appropriate for criminal and juvenile justice with the modification of the language in the tool to reflect criminal and juvenile justice. <a href="#fnref36" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn37" class="footnote-item">
            The NCJA Center for Justice Planning (n.d.). <em>SWOT Analysis</em>. Washington, DC: US Department of Justice, Bureau of Justice Assistance, and the National Criminal Justice Association. Retrieved from <a href="http://bit.ly/2gkcLUV">http://bit.ly/2gkcLUV</a>.
                <a href="#fnref37" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn38" class="footnote-item">
            Crime and Justice Institute and Guevara, M., Loeffler-Cobia, J., Rhyne, C., & Sachwald, J. (2011). <em>Putting the pieced together: Practical strategies for implementing evidence-based practices.</em> Washington, DC: National Institute
                of Corrections. <a href="#fnref38" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn39" class="footnote-item">
            Crime and Justice Institute and Guevara, M., Loeffler-Cobia, J., Rhyne, C., & Sachwald, J. (2011). <em>Putting the pieced together: Practical strategies for implementing evidence-based practices.</em> Washington, DC: National Institute
                of Corrections. <a href="#fnref39" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn40" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref40" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn41" class="footnote-item">
            Bertram, R. M., Blasé, K. A., & Fixsen, D. L. (2013). <em>Improving programs and outcomes: Implementation frameworks 2013.</em> Houston, TX.: Draft for Building the Research & Practice Gap Symposium in Houston, April 5th-6th, 2013.
                <a href="#fnref41" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn42" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice,</em> 19(5), 531-540.; Fixsen, D. L., Naoom, S. F., Blase, K. A., Friedman, R. M. & Wallace, F. (2005).<em>Implementation research: A synthesis of the literature</em>.
                Tampa, FL: University of South Florida, Louis de la Parte Florida Mental Health Institute, The National Implementation Research Network (FMHI Publication #231). <a href="#fnref42" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn43" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref43" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn44" class="footnote-item">
            Alexander, M. (2011). Applying implementation research to improve community corrections: Making sure that “new” thing sticks. <em>Federal Probation</em>, 75(2), 47-51. <a href="#fnref44" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn45" class="footnote-item">
            Makarios, M., Lovins, L., Latessa, E. J., & Smith, P. (2016). Staff quality and treatment effectiveness: An examination of the relationship between staff factors and the effectiveness of correctional programming. <em>Justice Quarterly,</em>                33(2), 348-367. <a href="#fnref45" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn46" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487.; Fixsen, D.L., Blase, K.A., Naoom, S.F.,
                & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540. <a href="#fnref46" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn47" class="footnote-item">
            Schoenwald, S. K., Sheidow, A. J., & Letourneau, E. J. (2004). Toward effective quality assurance in evidence-based practice: Links between expert consultation, therapist fidelity, and child outcomes. <em>Journal of Clinical Child and Adolescent Psychology,</em>                33(1), 94-104. <a href="#fnref47" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn48" class="footnote-item">
            Schwalbe, C. S., Oh, H. Y., & Zweben, A. (2014). Sustaining motivational interviewing: A meta-analysis of training studies. <em>Addiction</em>, 109, 1287-1294. <a href="#fnref48" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn49" class="footnote-item">
            Rogers, R. W., Wellins, R. S., & Connor, D. R. (2002). <em>White Paper – The power of realization.</em> BrRetreived from <a href="http://bit.ly/2yxwKsN">http://bit.ly/2yxwKsN</a>. ; Paparozzi, M. A., & Guy, R. (2013). The trials and
                tribulations of implementing what works: Training rarely trumps values. Federal Probation, 77(2). Retrieved from <a href="http://bit.ly/2yzLcBi">http://bit.ly/2yzLcBi</a>. <a href="#fnref49" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn50" class="footnote-item">
            Labrecque, R. M., & Smith, P. (2015). Does training and coaching matter? An 18-month evaluation of a community supervision model. <em>Victims & Offenders</em>, 12, 233-252.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015).
                Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref50" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn51" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref51" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn52" class="footnote-item">
            Fixsen, D. L., Naoom, S. F., Blase, K. A., Friedman, R. M. & Wallace, F. (2005).<em>Implementation research: A synthesis of the literature</em>. Tampa, FL: University of South Florida, Louis de la Parte Florida Mental Health Institute,
                The National Implementation Research Network (FMHI Publication #231). <a href="#fnref52" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn53" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540. <a href="#fnref53" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn54" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487.; Fixsen, D.L., Blase, K.A., Naoom, S.F.,
                & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540. <a href="#fnref54" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn55" class="footnote-item">
            Henderson, C. E., Taxman, F. S., & Young, D. W. (2007). A Rasch model analysis of evidence-based treatment practices used in the criminal justice system. <em>Drug and Alcohol Dependence</em>, 93, 163-175.; Farrell, J. L., Young, D. W.,
                & Taxman, F. S. (2011). Effects of organizational factors on use of juvenile supervision practices. <em>Criminal Justice and Behavior</em>, 38(6), 565-583. <a href="#fnref55" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn56" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref56" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn57" class="footnote-item">
            This is just one resource an organization can use to develop CQI in order to integrate frequent reporting and information related to organizational and programmatic functioning. The <a href="http://www.ihi.org/resources/Pages/HowtoImprove/default.aspx">Plan-Do-Study-Act (PDSA)</a>                is also a great resource for implementing quality assurance planning and processes. <a href="#fnref57" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn58" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref58" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn59" class="footnote-item">
            Carey, M. (2010). <em>Coaching packet: Continuous quality improvement.</em> Silver Springs, MD: The Center for Effective Public Policy. <a href="#fnref59" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn60" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref60" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn61" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540. <a href="#fnref61" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn62" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref62" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn63" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref63" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn64" class="footnote-item">
            Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref64" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn65" class="footnote-item">
            National Implementation Research Network. (n.d.). <em>Leadership</em>. Chapel Hill, NC: Author. Retrieved from <a href="http://unc.live/2gsOJuw">http://unc.live/2gsOJuw</a>. <a href="#fnref65" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn66" class="footnote-item">
            National Implementation Research Network. (n.d.). <em>Leadership</em>. Chapel Hill, NC: Author. Retrieved from <a href="http://unc.live/2gsOJuw">http://unc.live/2gsOJuw</a>. <a href="#fnref66" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn67" class="footnote-item">
            National Implementation Research Network. (n.d.). <em>Leadership</em>. Chapel Hill, NC: Author. Retrieved from <a href="http://unc.live/2gsOJuw">http://unc.live/2gsOJuw</a>. <a href="#fnref67" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn68" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref68" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn69" class="footnote-item">
            Fixsen, D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and
                outcomes: Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref69" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn70" class="footnote-item">
            Bertram, R. M., Schaffer, P. , & Charnin, L. (2014). Changing organization culture: Data driven participatory evaluation and revision of wrapround implementation. <em>Journal of Evidence-Based Social Work</em>, 11(1-2), 18-29.; Fixsen,
                D.L., Blase, K.A., Naoom, S.F., & Wallace, F. (2009). Core implementation components. <em>Research on Social Work Practice</em>, 19(5), 531-540.; Bertram, R. M., Blase, K. A., & Fixsen, D. L. (2015). Improving programs and outcomes:
                Implementation frameworks and organization change. <em>Research on Social Work Practice</em>, 25(4), 477-487. <a href="#fnref70" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn71" class="footnote-item">
            Kemp, L. (2016). Adaptation and fidelity: A recipe analogy for achieving both in population scale implementation. <em>Prevention Science</em>, 17, 429-438. <a href="#fnref71" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn72" class="footnote-item">
            Winter, S. G., & Szulanski, G. (2001). Replication as strategy. <em>Organization Science</em>, 12(6), 730-743. <a href="#fnref72" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn73" class="footnote-item">
            McHugh, R. K., Murray, H. W., & Barlow, D. H. (2009). Balancing fidelity and adaptation in the dissemination of empirically-supported treatments: The promise of transdiagnostic interventions. <em>Behavior Research and Therapy</em>, 47(<em>11</em>),
                946-953. <a href="#fnref73" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn74" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161. <a href="#fnref74" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn75" class="footnote-item">
            Kemp, L. (2016). Adaptation and fidelity: A recipe analogy for achieving both in population scale implementation. <em>Prevention Science</em>, 17, 429-438. <a href="#fnref75" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn76" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161. <a href="#fnref76" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn77" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161.; Kemp, L. (2016). Adaptation and fidelity: A recipe
                analogy for achieving both in population scale implementation. <em>Prevention Science</em>, 17, 429-438. <a href="#fnref77" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn78" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161. <a href="#fnref78" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn79" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161. <a href="#fnref79" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn80" class="footnote-item">
            Moore, J. E., Bumbarger, B. K., & Rhoades Cooper, B. (2013). Examining adaptations of evidence-based programs in natural contexts. <em>Journal on Primary Prevention</em>, 34, 147-161. <a href="#fnref80" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn81" class="footnote-item">
            Dusenbury, L., Brannigan, R., Hansen, W. B., Walsh, J., & Falco, M. (2005). Quality of implementation: Developing measures crucial to understanding the diffusion of preventive interventions. <em>Health Education Research</em>, 20, 308-313.;
                Dusenbury, L., Brannigan, R., Falco, M., & Hansen, W. B. (2003). A review of research on fidelity of implementation: Implications for drug abuse prevention in school settings. <em>Health Education Research</em>, 18, 237-256. <a href="#fnref81" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn82" class="footnote-item">
            Cooper, B. R., Shrestha, G., Hyman, L., & Hill, L. (2016). Adaptations in a community-based family intervention: Replication of two coding schemes. <em>The Journal of Primary Prevention</em>, 37(1), 33-52.; Supplee, L., & Metz, A.
                (2015). Opportunities and challenges in evidence-based social policy. <em>Social Policy Report</em>, 28 (4). <a href="#fnref82" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn83" class="footnote-item">
            Cooper, B. R., Shrestha, G., Hyman, L., & Hill, L. (2016). Adaptations in a community-based family intervention: Replication of two coding schemes. <em>The Journal of Primary Prevention</em>, 37(1), 33-52.; Supplee, L., & Metz, A.
                (2015). Opportunities and challenges in evidence-based social policy. <em>Social Policy Report</em>, 28 (4). <a href="#fnref83" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn84" class="footnote-item">
            Kemp, L. (2016). Adaptation and fidelity: A recipe analogy for achieving both in population scale implementation. <em>Prevention Science</em>, 17, 429-438. <a href="#fnref84" class="footnote-backref">↩︎</a>
        </li>
        <li id="fn85" class="footnote-item">
            Backer, T. (2002). *Finding the balance: Program fidelity and adaptation in substance abuse prevention: A state-of-the-art review. *Washington, DC: Center for Substance Abuse Prevention, Substance Abuse and Mental Health Services Administration,
                U.S. Department of Health and Human Services.; Henderson, C. E., Taxman, F. S., & Young, D. W. (2007). A Rasch model analysis of evidence-based treatment practices used in the criminal justice system. <em>Drug and Alcohol Dependence</em>,
                93, 163-175.; Liddle, H. A., Rowe, C. L., Quille, T. J., Dakof, G. A., Mills, D. S., Sakran, E., & Biaggi, H. (2002). Transporting a research-based adolescent drug treatment into practice. <em>Journal of Substance Abuse Treatment</em>,
                22(4), 231-243.; Schoenwald, S. K., & Hoagwood, K. (2001). Effectiveness, transportability, and dissemination of interventions: What matters when? <em>Psychiatric Services</em>, 52(9) 1190-1197. <a href="#fnref85" class="footnote-backref">↩︎</a>
        </li>
    </ol>
</section>