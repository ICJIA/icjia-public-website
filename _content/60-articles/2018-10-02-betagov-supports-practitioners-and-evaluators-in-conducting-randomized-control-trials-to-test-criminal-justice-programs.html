---
title: BetaGov Supports Practitioners and Evaluators in Conducting Randomized Control Trials to Test Criminal Justice Programs
_template: article
authorviz: 'false'
dashboard: 'false'
paragraphType: indent
tabviz: 'false'
tab1: Article
tab2: PDF Version
area:
  - Reentry
pubtype:
  - Article
teaser: This article explores the use of randomized control trials (RCTs) to test criminal justice programs to measure effectiveness. BetaGov is a group that offers assistance to practitioners and researchers to conduct RCTs. An example of such a collaboration between ICJIA and Betagov is offered along with lessons learned.
summary: 'This article explores the use of randomized control trials  (RCTs) to test criminal justice programs to measure effectiveness. BetaGov is a group that offers assistance to practitioners and researchers to conduct RCTs. An example of such a collaboration between ICJIA and Betagov is offered along with lessons learned.'
authors:
  - Justin Escamilla
  - Jessica Reichert
  - Maureen Hillhouse
  - Angela Hawken
keywords:
  - research
  - randomized control trial
  - testing
  - RCT
  - evidence-based
  - Reentry
  - experiments
  - social science
  - evaluation
leftColumn: |
  <h5 style="color: rgb(68, 68, 68); text-transform: uppercase; padding-bottom: 8px; font-weight: 900; margin-bottom: 10px; margin-left: 10px; border-bottom-color: rgb(204, 204, 204); border-bottom-width: 1px; border-bottom-style: solid;">Article Contents</h5>
  <ul class="nav">
  	<li><a class="scrollclass" data-target="#content-wrap">Introduction</a></li>
  	<li><a class="scrollclass" data-target="#randomized-control-trials-why-are-they-important">Randomized Control Trials: Why Are They Important?</a></li>
      <li><a class="scrollclass" data-target="#what-is-betagov">What Is BetaGov?</a></li>
      <li><a class="scrollclass" data-target="#a-betagov-collaboration-example-randomized-control-trial-of-an-illinois-reentry-program">A BetaGov Collaboration Example: Randomized Control Trial of an Illinois Reentry Program</a></li>    
      <li><a class="scrollclass" data-target="#lessons-learned-from-our-randomized-control-trial">Lessons Learned from Our Randomized Control Trial</a></li>        
  </ul>
splash: /assets/img/splash/BetaGovRCTarticle09-28-18_Stock-905715546.jpg
super_positive_keywords:
  - research
  - randomized control trial
  - testing
  - RCT
  - evidence-based
  - Reentry
  - experiments
  - social science
  - evaluation
---
<!-- <h1 id="betagov-supports-practitioners-and-evaluators-in-conducting-randomized-control-trials-to-test-criminal-justice-programs">BetaGov Supports Practitioners and Evaluators in Conducting Randomized Control Trials to Test Criminal Justice Programs</h1> -->

<style>

.style1 {
    background: #536878; 
	font-family: "sans serif", Arial;
	font-size: 14px;
}
</style>

<!-- <pre><code>THIS IS THE FINAL PUBLISHED ARTICLE IN TRANSLATIONAL CRIMINOLOGY, THE MAGAZINE OF THE 
CENTER FOR EVIDENCE-BASED CRIME POLICY, GEORGE MASON UNIVERSITY, AVAILABLE AT: 
HTTPS://CEBCP.ORG/WP-CONTENT/TCMAGAZINE/TC15-FALL2018. THE ARTICLE HAS BEEN 
REPRINTED WITH PERMISSION.
</code></pre> -->

<br>
                <div class="container-fluid style1" style="margin: 0; padding: 0;">
                <div class="row" style="background-color:#D9D9D9">
                <div class="col-xs-12 " style="style1">
<br>
<strong>This article was originally published in <em>Translational Criminology, the Magazine of the Center for Evidence-Based Crime Policy</em>, George Mason University, available at: <a href="https://cebcp.org/wp-content/TCmagazine/TC15-Fall2018"> https://cebcp.org/wp-content/TCmagazine/TC15-Fall2018</a>. Reprinted with permission.</strong><br>
<br>
  </div>
</div>
</div>
<br>

<p>Many fields employ researchers and others who routinely conduct experiments using randomized control trials (RCTs) to test daily operations to learn about and improve their work to increase success. However, fields in the public sector, including criminal
    justice, have lagged in experimentation. Testing is pervasive in medicine, marketing, and business as part of the standard process for decision-making and advancement. Amazon CEO Jeff Bezos said his company’s success is “a function of how many experiments
    [they] do per year, per month, per week, per day.” <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> Medicine, marketing, and business leaders recognize how rare it is to get it right the first time, and they embrace failure as a learning
    opportunity.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> In the public sector, ethical concerns, practicality, and a lack of knowledge are some of the barriers to the use of RCTs.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p>
<p>Most public policies and practices—such as how we educate our children, deliver health care, rehabilitate convicted offenders, or house the homeless—have one thing in common: They have never been empirically tested. Conducting rigorous evaluation traditionally
    involves academic researchers and government funding and requires navigating through bureaucratic red tape that makes the research difficult to accomplish. As a result, many commonplace policies intended to make citizens smarter, safer, or healthier
    are based more on public perception or “business as usual” than on empirical data.</p>
<h2 id="randomized-control-trials-why-are-they-important">Randomized Control Trials: Why Are They Important?</h2>
<p>Randomized control trials are the scientific gold standard for program evaluation. Evaluators employ RCTs to measure program effectiveness by isolating the effects of programmatic conditions from other factors that may contribute to varying outcomes among
    similar groups, such as systematic bias in program participation.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> RCTs can help ensure government policies are effective and will help those who need them most.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>    Programs that undergo RCT-based evaluations also become more attractive to grantmakers and investors who value scientific evidence. Robust and stable funding streams enable program adoption, expansion, and long-term improvements.</p>
<h2 id="what-is-betagov">What Is BetaGov?</h2>
<p>BetaGov is a multidisciplinary group of academic and practice-experienced researchers who help agencies explore possible improvements in policies and practices in domains ranging from criminal justice to education. BetaGov’s approach is unconventional
    by design; BetaGov exists because conventional approaches to conceptualizing and conducting research to test policies and practices can be inefficient, and the results are often irrelevant to real-world practitioners and policymakers. BetaGov was created
    to promote scientific evaluations of policies and practices via RCTs and to make these assessments the norm rather than the exception. BetaGov’s mission is to help agencies, policymakers, and others develop, conduct, analyze, and share research on
    policies and practices that affect the public they serve. Guidance from BetaGov—provided at no cost to the end user—facilitates design and implementation of research conducted by service agencies and departments at all government levels. The goal
    is to significantly increase the pace of learning about policies pertaining to health services, social services, criminal justice, education, and other domains; identify promising innovations; and identify policies and practices that are inefficient
    or ineffective.</p>
<p>With BetaGov’s guidance and assistance, practitioners can carry out their own RCTs. Being able to design and implement a trial without funding and often without regulatory hurdles means that the trial can be more quickly conducted and completed. The private
    sector has long relied on simple, pragmatic RCTs to improve efficiency and performance; BetaGov promotes the use of these same techniques to inform policy solutions for the most challenging health and social problems.</p>
<h2 id="a-betagov-collaboration-example-randomized-control-trial-of-an-illinois-reentry-program">A BetaGov Collaboration Example: Randomized Control Trial of an Illinois Reentry Program</h2>
<p>The Illinois Criminal Justice Information Authority (ICJIA) is a state government agency that administers federal criminal justice grants and serves as the state’s Statistical Analysis Center for criminal justice research. Researchers from ICJIA were
    recruited to evaluate a newly established 2018 prisoner reentry program in Illinois. The program, Pathway to Enterprise for Returning Citizens (PERC), offers entrepreneurship training to individuals returning to Cook County, Illinois, communities
    from prison. The training focuses on how to start a business, offers a mentor for support, and provides an opportunity to obtain a small business loan. There is very little research on prior entrepreneurship reentry programs and none employing an
    RCT design.</p>
<p>The ICJIA researchers were tasked with measuring program benefits.They sought to collect program process information<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> and use an RCT to compare program outcomes, such as securing employment,
    reducing recidivism (arrest and reincarceration), and starting a business. As a government agency with research experience but few prior opportunities to apply an RCT, ICJIA partnered with BetaGov for assistance. BetaGov and ICJIA researchers scheduled
    regular conference calls to discuss evaluation components and RCT implementation. The team stratified applicants by prison release date and distance from PERC training agencies, and BetaGov completed the random assignment of program applicants into
    either the treatment group (PERC-trained) or the control condition (on parole with no PERC training). With BetaGov’s help, ICJIA researchers saved time while avoiding the appearance of possible bias in participant selection.</p>
<h2 id="lessons-learned-from-our-randomized-control-trial">Lessons Learned from Our Randomized Control Trial</h2>
<h3 id="practitioners-should-view-evaluation-as-a-valuable-tool">Practitioners Should View Evaluation as a Valuable Tool</h3>
<p>To seasoned practitioners with years of field experience, a formal evaluation guided by researchers may seem irrelevant. After all, most programs make adjustments over the course of development, and changes on the ground are easily recognized. However,
    even the sharpest practitioners may not recognize that complex external factors unrelated to programmatic decisions may be driving observable changes. The scientific method employed by RCTs can most effectively rule out external factors as the explanation
    for ground-level changes, which makes an RCT evaluation an efficient and accurate way to discover whether program activities achieve the desired outcomes. Accordingly, it saves time and resources otherwise spent on ineffective and unproven modifications.</p>
<p>Of course, researchers must acknowledge that practitioners often have more immediate needs. Short-term feedback, interim reports, program updates, and troubleshooting may be required long before the formal evaluation is completed. Researchers should confer
    with practitioners on their needs to see what can be immediately addressed and clearly communicate the project scope to manage expectations. The development of methodology, data collection, and other activities required by rigorous evaluation may
    take longer than what is naturally learned throughout normal operations. However, knowledge gained through an RCT evaluation will be based on empirically derived data and useful in the long-term.</p>
<h3 id="examine-a-programs-mission-and-goals">Examine a Program’s Mission and Goals</h3>
<p>For an evaluation to be a truly collaborative endeavor, the mission of a program must be reflected in the evaluation design and in the outcomes to be measured. Goals of funders, program leadership, staff, and other core stakeholders must be aligned and
    sufficiently understood to ensure meaningful research questions and valuable results. Establishing clear goals can be difficult for new programs and for programs lacking consistent direction from leaders, but an evaluation can be a great opportunity
    to define new goals or get reacquainted with original program goals. Researchers can help construct operational definitions to accurately measure the concepts most meaningful to program stakeholders.</p>
<p>Creating a logic model that maps how stakeholders are connected to program activities and how those activities are connected to main goals is a great way to align program partners. An honest, open-minded discussion about the feasibility of the program
    model should take place. If the purpose and scope of a program or its evaluation is not clear to key stakeholders before the evaluation begins, more organization and collaboration is needed. In the end, practitioners and researchers should be able
    to provide similar answers to the questions: “What does the program hope to achieve?” and “How will you know if that has been achieved?”</p>
<h3 id="know-how-to-address-ethical-concerns-and-design-issues">Know How to Address Ethical Concerns and Design Issues</h3>
<p>An RCT can be derailed by data collection barriers, mid-program adjustments, scheduling, and study participant retention issues. A plan should be in place to deal with these issues as they arise. However, sometimes the very nature of an RCT can be problematic.
    Some stakeholders may have concerns about randomly excluding people from a program meant to be helpful. However, assigning people to not receive a beneficial treatment or intervention can be acceptable under the right circumstances. No one sets out
    to create an ineffective program, let alone one that produces unintended negative consequences for its participants. Good intentions, however, do not make good programs. Program staff need to be open to the idea that their programs as currently administered
    may be ineffective, inefficient, or unhelpful, especially when their program is new or untested. The purpose of a rigorous evaluation is to determine whether a program does what it aims to do so that participants benefit as intended.</p>
<p>Researchers must keep the principles of ethical research in focus at all times and understand that scientific research has not always embodied these principles in the past.<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> Collaboration
    on RCTs will work best when researchers directly address practitioner concerns and can offer practical solutions to issues with random assignment when possible, including assigning from a waitlist or providing alternate programming over no programming.
    <sup
        class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> Finally, researchers should recognize when the logistics and context of a project are simply not conducive to an RCT, such as when quick results are paramount, precision or causal inference are not desired,
        proper planning is impossible, or if an RCT would introduce too many complications for a budding program to handle.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<h3 id="be-resilient-and-continue-to-advocate-for-rcts-in-criminal-justice">Be Resilient and Continue to Advocate for RCTs in Criminal Justice</h3>
<p>Not every part of an RCT evaluation will go smoothly, but when researchers are committed to a solid evaluation plan, adjustments can be made to account for arising issues. This can be easier with the assistance of BetaGov and other associations whose
    expertise and experiences allow them to help solve evaluation problems associated with unexpected events, strained resources, and knowledge gaps. With supportive services that speak the language of both practice and research, researchers should feel
    encouraged to advocate for more RCTs in the public sector and examine the questions that programs really need answered. Only rigorous evaluations that ask meaningful questions will result in evidence-based programming and sustainable improvement.</p>

<br>
                <div class="container-fluid style1" style="margin: 0; padding: 0;">
                <div class="row" style="background-color:#D9D9D9">
                <div class="col-xs-12 " style="style1">
<br>
This project was supported by Grant #16-DJ-BX-0083, awarded to the Illinois Criminal Justice Information Authority by the U.S. Department of Justice Office of Justice Programs’ Bureau of Justice Assistance. Points of view or opinions contained within this document are those of the authors and do not necessarily represent the official position or policies of the U.S. Department of Justice.<br>
<hr>
<strong>Suggested citation:</strong> Escamilla, J., Reichert, J., Hillhouse, M., & Hawken, A. (2018). BetaGov supports practitioners and evaluators in conducting randomized control trials to test criminal justice programs. <em>Translational Criminology</em>, 15, 29-31.<br>
<br>
  </div>
</div>
</div>
<br>


<hr class="footnotes-sep">
<section class="footnotes">
    <ol class="footnotes-list">
        <li id="fn1" class="footnote-item">
            <p>Simmons, M. (2017). Forget the 10,000-hour rule; Edison, Bezos, and Zuckerberg follow the 10,000 experiment rule. <em>Medium</em>. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn2" class="footnote-item">
            <p>Edmondson, A. C. (2011). Strategies for learning from failure. <em>Harvard Business Review</em>. Retrieved from <a href="http://bit.ly/2OAqysS">http://bit.ly/2OAqysS</a>                <a href="#fnref2" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn3" class="footnote-item">
            <p>Shadish, W. R., Cook, T. D., & Campbell, D. T. (2001). <em>Experimental and quasi-experimental designs for generalized causal inference</em>. Boston, MA: Houghton Mifflin. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn4" class="footnote-item">
            <p>Shadish, W. R., Cook, T. D., & Campbell, D. T. (2001). <em>Experimental and quasi-experimental designs for generalized causal inference</em>. Boston, MA: Houghton Mifflin. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn5" class="footnote-item">
            <p>Buck, S., & McGee, J. (2015, July). <em>Why government needs more randomized controlled trials: Refuting the myths</em>. Laura and John Arnold Foundation. Retrieved from <a href="http://bit.ly/2ybi0OS">http://bit.ly/2ybi0OS</a>                 <a href="#fnref5" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn6" class="footnote-item">
            <p>Data included intake data, pre-tests and post-tests, exit surveys, mentor surveys, focus groups, and interviews. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn7" class="footnote-item">
            <p>Mandal, J., Acharya, S., & Parija, S. C. (2011). Ethics in human research. Tropical Parasitology, 1(1), 2–3. <a href="http://bit.ly/2Ov2eJ7">http://bit.ly/2Ov2eJ7</a> The Belmont Report. (1979, April).
                Washington, DC: Department of Health, Education, and Welfare. Retrieved from <a href="http://bit.ly/2zMBdZy">http://bit.ly/2zMBdZy</a>                <a href="#fnref7" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn8" class="footnote-item">
            <p>Shadish, W. R., Cook, T. D., & Campbell, D. T. (2001). <em>Experimental and quasi-experimental designs for generalized causal inference</em>. Boston, MA: Houghton Mifflin. <a href="#fnref8" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn9" class="footnote-item">
            <p>Shadish, W. R., Cook, T. D., & Campbell, D. T. (2001). <em>Experimental and quasi-experimental designs for generalized causal inference</em>. Boston, MA: Houghton Mifflin. <a href="#fnref9" class="footnote-backref">↩︎</a></p>
        </li>
    </ol>
</section>





<!-- Article Contents links under the side section
<h5 style="color: rgb(68, 68, 68); text-transform: uppercase; padding-bottom: 8px; font-weight: 900; margin-bottom: 10px; margin-left: 10px; border-bottom-color: rgb(204, 204, 204); border-bottom-width: 1px; border-bottom-style: solid;">Article Contents</h5>
<ul class="nav">
	<li><a class="scrollclass" data-target="#content-wrap">Introduction</a></li>
	<li><a class="scrollclass" data-target="#randomized-control-trials-why-are-they-important">Randomized Control Trials: Why Are They Important?</a></li>
    <li><a class="scrollclass" data-target="#what-is-betagov">What Is BetaGov?</a></li>
    <li><a class="scrollclass" data-target="#a-betagov-collaboration-example-randomized-control-trial-of-an-illinois-reentry-program">A BetaGov Collaboration Example: Randomized Control Trial of an Illinois Reentry Program</a></li>    
    <li><a class="scrollclass" data-target="#lessons-learned-from-our-randomized-control-trial">Lessons Learned from Our Randomized Control Trial</a></li>        
</ul> -->
