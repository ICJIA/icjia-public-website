---
title: 'Demystifying Program Evaluation in Criminal Justice: A Guide for Practitioners'
_template: article
splash: /assets/img/splash/adult-back-view-data-1181345-min.jpg
authors:
  - Jessica Reichert
  - Alysson Gatens
authorviz: 'false'
area:
  - evaluation
pubtype:
  - article
keywords:
  - research
  - evidence-based
  - social science
  - evaluation
teaser: Evaluation in criminal justice is vital to improving program effectiveness, increasing efficiency, and improving public safety. This article helps demystify the evaluation process and offers practical advice for practitioners endeavoring to evaluate a program. Processes for internal evaluation and external evaluation partnerships are described. Criminal justice practitioners are encouraged to understand the research process and conduct evaluations of their programs and practices.
summary: Evaluation in criminal justice is vital to improving program effectiveness, increasing efficiency, and improving public safety. This article helps demystify the evaluation process and offers practical advice for practitioners endeavoring to evaluate a program. Processes for internal evaluation and external evaluation partnerships are described. Criminal justice practitioners are encouraged to understand the research process and conduct evaluations of their programs and practices.
dashboard: 'false'
paragraphType: indent
tabviz: 'true'
tab1: Overview
tab2: PDF
pdf_uploads:
  - 
    type: report_set
    title: 'Demystifying Program Evaluation in Criminal Justice: A Guide for Practitioners'
    pdf: /assets/articles/Demystifying Evaluation.pdf
    reportType: Full Report
    reportCoverImage: ""
leftColumn: |
  <h5 style="color: rgb(68, 68, 68); text-transform: uppercase; padding-bottom: 8px; font-weight: 900; margin-bottom: 10px; margin-left: 10px; border-bottom-color: rgb(204, 204, 204); border-bottom-width: 1px; border-bottom-style: solid;">Article Contents</h5>
  <ul class="nav">
  	<li><a class="scrollclass" data-target="#what-is-program-evaluation">What is Program Evaluation?</a></li>
  	<li><a class="scrollclass" data-target="#when-should-i-start-my-program-evaluation">When Should I Start my Program Evaluation?</a></li>
  	<li><a class="scrollclass" data-target="#how-should-i-start-my-program-evaluation">How Should I Start My Program Evaluation?</a></li>
  	<li><a class="scrollclass" data-target="#what-kind-of-evaluations-can-be-done">What Kind of Evaluations Can be Done?</a></li>
  	<li><a class="scrollclass" data-target="#putting-it-all-together-using-what-is-learned-from-a-program-evaluation">Putting it All Together: Using What is Learned from a Program Evaluation</a></li>
  	<li><a class="scrollclass" data-target="#conclusion">Conclusion</a></li>
  </ul>
tab3: Resources
tab3Content: |
  <h2 id="resources">Resources</h2>
  • <a href="https://www.americansebp.org/">American Society of Evidence-Based Policing (ASEBP)</a><br>
  • <a href="http://www.betagov.org/">Betagov</a><br>
  • <a href="https://www.crimesolutions.gov/">Crimesolutions.gov</a><br>
  • <a href="https://www.campbellcollaboration.org/">Campbell Collaboration</a><br>
  • <a href="https://www.cdc.gov/eval/index.htm">Centers for Disease Control and Prevention, Program Performance and Evaluation Office</a><br>
  • <a href="http://coalition4evidence.org/">Coalition for Evidence-based Policy</a><br>
  • <a href="http://www.cochrane.org/">Cochrane Collaboration</a><br>
  • <a href="https://whatworks.csgjusticecenter.org/">Council of State Governments: What Works in Reentry Clearinghouse</a><br>
  • <a href="http://www.jrsa.org/projects/ebp_briefing_paper_april2014.pdf">An Introduction to Evidence-Based Practices (JRSA)</a><br>
  • <a href="https://www.ojjdp.gov/mpg">OJJDP Model Programs Guide</a><br>
  • <a href="http://www.samhsa.gov/nrepp">SAMHSA’s National Registry of Evidence-based Programs and Practices</a><br>
  • <a href="http://www.colorado.edu/cspv/">UC-Boulder’s Center for the Study of Prevention of Violence: Blueprints</a><br>
  • <a href="http://www.wsipp.wa.gov/">Washington State Institute for Public Policy</a>
---
<style>

.callout {
    width: 400px;
	padding: 15px;
}

.right {
      float: right; 
	  margin-left: 10px; 
  	  margin-bottom: 10px
	  padding-top: 10px
}

.style1 {
    background: #536878; 
	font-family: "sans serif", Arial;
	font-size: 18px;
}

.style3 {color: #FFFFFF}
}
</style>

<!--<center>
<p>
	<a class="btn btn-primary" href="http://www.icjia.state.il.us/articles/protecting-participants-of-social-science-research">Part II : Protecting Participants of Social Science Research<i class="fa fa-caret-right"></i>
	</a>
</p>
</center>-->

<div class="well text-center" style="margin-top: 30px; margin-bottom: 30px">
<h3 class="h5" id="first-in-a-series" style="font-weight: 700; font-family: Lato, sans-serif">First in a series. Read <a href="http://www.icjia.state.il.us/articles/protecting-participants-of-social-science-research">Part II : Protecting Participants of Social Science Research</a>.</h3>
</div>

<h2 id="what-is-program-evaluation">What is Program Evaluation?</h2>
<p>Program evaluation that is rooted in science is critical for criminal justice.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> Criminal justice programs should engage in evaluation to provide proof effectiveness and legitimacy and justify taxpayer support.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> According to Weisburd (2003), criminal justice researchers have an ethical and professional “obligation to provide valid answers to questions about the effectiveness of treatments, practices, and programs.”<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> In recent years, policy-makers and funders have focused on investing in evidence-based, criminal justice programs.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> In addition, a movement toward evidence-based policymaking has created an increased demand for quality research evaluation.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> An evaluation can allow a program to make improvements, better secure funding, and potentially expand.</p>
<p>Research is often conducted by academics at universities, think tanks, government agencies, and nonprofits. However, external research can be time consuming and costly. In a review of the top five academic criminal justice journals over five years, only 5.2 percent of articles were evaluation research.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></p>

<div class="callout right style1 style3">
<div ><br>Program Evaluation and Funding<br> 
<hr>
  Criminal justice programs are often costly and<br> 
  practitioners often compete to secure funds<br>
  from county boards, city councils, federal and<br>
  state granting agencies, and private<br>
  foundations. Rigorously evaluated programs<br>
  can distinguish themselves from the pack by <br>
  showing evidence of program effectiveness.<br /> 
   <p></p></div>
</div>


<h3 id="when-should-i-start-my-program-evaluation">When Should I Start my Program Evaluation?</h3>
<p>Evaluation planning ideally starts during the initial stages of program development.<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> This encourages program staff to clearly define goals and objectives and gather data to document program implementation and outcomes.</p>
<p>Practitioners should hold off on conducting outcome evaluations until their programs have operated long enough to adequately address initial implementation issues and produced enough data to rigorously analyze outcomes.<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> Some researchers advocate for an “evaluability” assessment, which provides information on whether a program has reached the appropriate stage for systematic evaluation. Evaluability assessments also can prevent a premature undertaking of an impact evaluation.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<p>It is worth noting that as a program embarks on an evaluation, staff and stakeholders may be nervous of negative findings about the program. Researchers conducting the evaluation may encounter their own challenges, such as isolating the effects of the program, competing interests of stakeholders, attrition, and resource constraints. However, these challenges should not impede evaluations which should be viewed as opportunities for programs to learn, improve, and grow.<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<h3 id="how-should-i-start-my-program-evaluation">How Should I Start My Program Evaluation?</h3>
<p><h4 id="develop-logic-model">Develop a Logic Model</h4> Logic models provide a graphical depiction of a program that outlines and links relationships among program activities, outputs, and outcomes.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup> <em>Figure 1</em> depicts a logic model and explains each component. A logic model can be a first step in the development and management of a program and can guide evaluation activities.</p>
<div class="article-figure">
    <h4  align="center"  id="figure-1">Figure 1</h4>
    <h4  align="center" id="logic-model-example">Logic Model Example  <sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></h4>

   <div class="text-center " style="margin-top: 15px; margin-bottom: 15px;">
  <img
    src="/assets/img/articles/figure1_06042019.PNG"
    class="img-responsive center-block"
  />
  </div>

    <h6 id="source-national-institute-of-corrections">Source: National Institute of Corrections</h6>
</div>
<p><h4 id="develop-research-questions">Develop research questions</h4> Research questions will guide program evaluation and help outline goals of the evaluation. Research questions should align with the program’s logic model and be measurable.<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> The questions also guide the methods employed in the collection of data, which may include surveys, qualitative interviews, field observations, review of data collected by the program, and analysis of administrative records. Finally, questions should be tailored to how the resulting information will be used (e.g. internal practitioners looking to incrementally improve the program, or external funders determining program continuation).<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup></p>
<h3 id="what-kind-of-evaluations-can-be-done">What Kind of Evaluations Can be Done?</h3>
<p>An evaluation can focus on certain aspects of a program and to what extent they impact outcomes. The two main evaluation types— process and outcome evaluations—focus on specific program areas and are equally important.<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> When considering the kind of evaluation needed, program administrators and evaluators should take an inventory of available resources, including staff time, to conduct or assist in an evaluation.</p>

<div class="article-figure">
    <div class="text-center " style="margin-top: 15px; margin-bottom: 15px;">
  <img
    src="/assets/img/articles/image1 - processandoutcomeevaluations.PNG"
    class="img-responsive center-block"
  />
  </div>

    <h6 id="rethinking-program-fidelity-for-criminal-justice">Sources: Miller, J. M., & Miller, H. V. (2015). Rethinking program fidelity for criminal justice. <em>Criminology & Public Policy</em>, <em>14</em>(2), 339-349; Rossi, P. H., Lipsey, M. W., & Freeman, H. E. (2004). <em>Evaluation: A systematic approach</em>. (7th ed.). Thousand Oaks, CA: Sage Publications, Inc.</h6>

</div>


<p><h4 id="research-design-options-for-outcome-evaluations">Research design options for outcome evaluations</h4> The value of an outcome evaluation is directly related to what can and cannot be concluded so the most rigorous evaluation option should be employed.<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> In research, outcome evaluations that incorporate randomized control trials, where participants are randomly assigned to an experimental group or a control group, are the gold standard.<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> The groups are differentiated by the program or practice (outcome variable) being studied; the control group may receive alternative programming or no programming. This method allows evaluators to isolate the impact of the program on participants.</p>
<p>An alternative is quasi-experimental design which compares outcomes of program participants (the treatment group) with a similar group of individuals with which to compare (comparison group). The individuals are not randomly assigned, but the design allows for valid causal inferences, despite some drawbacks.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> The lack of random assignment causes non-equivalent test groups, which can limit the generalizability of the results, reduce internal validity, and restrict definitive conclusions about causality.<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> Biased evaluation findings from individual studies may under- or overrepresent the effects of the program.<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup></p>
<div align="left" class="well" style="margin-top: 20px; font-size: 20px">
    <h4 align="center" id="ethical-considerations-in-evaluation">Ethical Considerations in Evaluation</h4>

<hr>
    <p>In addition to considering sound methods, careful examination of how criminal justice research is conducted based on research ethics and standards is important. The evaluator has a responsibility to maintain integrity, balance, and fairness in study design and data collection methods and a duty to respect the security and dignity of all parties involved in the evaluation project. If the evaluation findings on an agency’s work are to be published and the agency receives federal funding, the research must follow the code of federal regulations for the protection of human subjects. The regulations outline procedures including approval of the research study from an institutional review board.<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup></p>

</div>

<h3 id="putting-it-all-together-using-what-is-learned-from-a-program-evaluation">Putting It All Together: Using What is Learned from a Program Evaluation</h3>
<p>As Escamilla and colleagues (2018) noted,</p>
<blockquote>
    <p>To seasoned practitioners with years of field experience, a formal evaluation guided by researchers may seem irrelevant. After all, most programs make adjustments over the course of development and changes on the ground are easily recognized. However, even the sharpest practitioners may not recognize that complex external factors unrelated to programmatic decisions may be driving observable changes.<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></p>
</blockquote>
<p>An evaluation can provide insight and offer recommendations to improve programs. Rigorous evaluation research can assess not only whether the target problem was reduced, but determine if the program <em>caused</em> the problem to be reduced.<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> Evaluation findings can be used in many ways, including:</p>
<ul>
    <li>Demonstrating the effectiveness of your program.</li>
    <li>Identifying ways to improve your program.</li>
    <li>Modifying program implementation and operations.</li>
    <li>Demonstrating accountability to program stakeholders and funders.</li>
    <li>Justifying funding and potentially aid in securing additional funding.<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></li>
</ul>
<p>Findings should be disseminated in documents or reports that can help other jurisdictions and programs learn from the evaluation. To maximize utilization of evaluation findings, the evaluator may tailor the report style to the intended audience (e.g. academic journal article, technical report, executive summary).<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup> The evaluation also may include information for the user about how the findings can be best employed and disseminated to additional constituencies.<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup></p>
<p><em>Figure 2</em> depicts the steps of an evaluation, from logic model creation through sharing findings and making programmatic adjustments.</p>
<div class="article-figure">
    <h4 align="center" id="figure-2">Figure 2</h4>
    <h4 align="center" id="steps-in-program-evaluation">Steps in Program Evaluation  <sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup></h4>
  
   <div class="text-center " style="margin-top: 15px; margin-bottom: 15px;">
  <img
    src="/assets/img/articles/figure2_06042019.PNG"
    class="img-responsive center-block"
  />
  </div>

    <h6 id="source-centers-for-disease-control-and-prevention-1999">Source: Centers for Disease Control and Prevention, 1999</h6>
</div>
<h3 id="building-evidence-of-program-effectiveness">Building Evidence of Program Effectiveness</h3>
<p>Evaluation findings of similar programs can build a body of knowledge, leading to the model being deemed evidence-based. An evidence-based program has strong evidence that it is effective based on reliable and replicated research, while an evidence-informed practice has less evidence of its efficacy (<em>Figure 3</em>).<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup> There are numerous program models that have proliferated at a rate far beyond that supported by accompanying evaluation research.<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup> Syntheses of multiple evaluations of similar program concepts can be employed to guide program and policy planning efforts.<sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup> However, research evaluation is a continuous process that will always be needed. Many programs adapt evidence-based programs to meet the needs of their unique communities, participant populations, program policies, and organizational climates. Many programs may produce an impact in the short-term, however relatively few evaluations examine outcomes beyond a year following a program’s initiation. Therefore, continued research is needed on long-term outcomes to identify the sustained impact of programs and policies.</p>
<div class="article-figure">
    <h4 align="center" id="figure-3">Figure 3</h4>
    <h4 align="center" id="building-evidence-of-effectiveness">Building Evidence of Effectiveness  <sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup></h4>
  
   <div class="text-center " style="margin-top: 15px; margin-bottom: 15px;">
  <img
    src="/assets/img/articles/figure3_06042019.PNG"
    class="img-responsive center-block"
  />
  </div>


    <h6 id="source-corporation-for-national-and-community-service">Source: Corporation for National and Community Service</h6>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>The field of criminal justice has fallen behind other fields, such as medicine, marketing, and business, in its use of evaluation research to inform and improve programming.<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup> As a result, perception, anecdotal evidence, and “business as usual,” rather than rigorous, empirical testing, become influential in program development.<sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup> Rigorous evaluation is necessary to make informed decisions on program improvement and bring accountability to the utilization of limited resources.</p>
<p>Evaluation research can provide important information to influence the policymaking in the criminal justice field. Conflicting stakeholder interests on policy or ideology should be superseded by rigorous research.<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup></p>
<p>Evaluation also contributes to improved treatment and service provision given to individuals. Without rigorous evaluation, it is unknown whether a program is providing any benefit to its participants. Ineffective programs may even cause unintended harm to those who participate. There is increasing agreement that criminal justice programming and practices should be grounded in scientific research to the greatest extent possible.</p>

<div class="well" style="margin-top: 20px; font-size: 16px; background-color:#D9D9D9">
    <p>This project was supported by Grant #16-DJ-BX-0083, awarded to the Illinois Criminal Justice Information Authority by the U.S. Department of Justice Office of Justice Programs’ Bureau of Justice Assistance. Points of view or opinions contained within this document are those of the authors and do not necessarily represent the official position or policies of the U.S. Department of Justice.</p>
<hr>
    <p><strong>Suggested citation:</strong> Reichert, J., & Gatens, A. (2019). <em>Demystifying program evaluation in criminal justice: A guide for practitioners</em>. Chicago, IL: Illinois Criminal Justice Information Authority.</p>
</div>

<hr class="footnotes-sep">
<section class="footnotes">
    <ol class="footnotes-list">
        <li id="fn1" class="footnote-item">
            <p>Braga, A. A., & Weisburd, D. L. (2013). Editor’s introduction: Advancing program evaluation methods in Criminology and Criminal Justice. <em>Evaluation Review</em>, <em>37</em>(3-4), 163-169. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn2" class="footnote-item">
            <p>Janeksela, G. M. (1977). An evaluation model for criminal justice. <em>Criminal Justice Review</em>, <em>2</em>(2), 1-11. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn3" class="footnote-item">
            <p>Weisburd, D. (2003). Ethical practice and evaluation of interventions in crime and justice: The moral imperative for randomized trials. <em>Evaluation Review</em>, <em>27</em>(3), 336-354. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn4" class="footnote-item">
            <p>Fagan, A. A., & Buchanan, M. (2016). What works in crime prevention? Compaison and critical review of three crime prevention registries. <em>Criminology and Public Policy</em>, <em>15</em>(3), 617-649. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn5" class="footnote-item">
            <p>Leeuw, F. (2005). Trends and developments in program evaluation in general and criminal justice programs in particular. <em>European Journal on Criminal Policy and Research</em>, <em>11</em>, 233-258. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn6" class="footnote-item">
            <p>Tewksbury, R., DeMichele, M. T., & Miller, J. M. (2005). Methodological orientations of articles appearing in criminal justice’s top journals: Who publishes what and where. <em>Journal of Criminal Justice Education</em>, <em>16</em>(2), 265-279. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn7" class="footnote-item">
            <p>Tilley, N. (2004). Applying theory-driven evaluation to the British Crime Reduction Programme: The theories of the programme and of its evaluations. <em>Criminal Justice</em>, <em>4</em>(3), 255-276. <a href="#fnref7" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn8" class="footnote-item">
            <p>Rossi, P. H., Lipsey, M. W., & Freeman, H. E. (2004). <em>Evaluation: A systematic approach</em> (7th ed.). Thousand Oaks, CA: Sage Publications, Inc. <a href="#fnref8" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn9" class="footnote-item">
            <p>Wholey, J. S. (1987). Evaluability assessment: Developing program theory. <em>New Directions for Program Evaluation</em>, <em>33</em>, 77-92. <a href="https://doi.org/10.1002/ev.1447">https://doi.org/10.1002/ev.1447</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn10" class="footnote-item">
            <p>Weisburd, D., Lum, C. M., & Petrosino, A. (2001). Does research design affect study outcomes in criminal justice?. <em>The Annals of the American Academy of Political and Social Science</em>, <em>578</em>(1), 50-70. <a href="#fnref10" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn11" class="footnote-item">
            <p>Bureau of Justice Assistance. (n.d.) <em>Center for research partnerships and program evaluation</em>. Retrieved from <a href="http://bit.ly/2Xeg1EU">http://bit.ly/2Xeg1EU</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn12" class="footnote-item">
            <p>National Institute of Corrections. (n.d.) <em>A framework for evidence-based decision making in local criminal justice systems</em>. Retrieved from <a href="http://bit.ly/2XnoQfX">http://bit.ly/2XnoQfX</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn13" class="footnote-item">
            <p>Corporation for National and Community Service. (n.d.) <em>How to develop the right research questions for program evaluation</em>. <a href="#fnref13" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn14" class="footnote-item">
            <p>Rossi, P. H., Lipsey, M. W., & Freeman, H. E. (2004). <em>Evaluation: A systematic approach</em> (7th ed.). Thousand Oaks, CA: Sage Publications, Inc. <a href="#fnref14" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn15" class="footnote-item">
            <p>Harrell, A., Burt, M., Hatry, H., Rossman, S., Roth, J, & Sabol, W. (1996). <em>Evaluation strategies for human services programs: A guide for policymakers and providers</em>. Washington, DC: The Urban Institute. <a href="#fnref15" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn16" class="footnote-item">
            <p>Braga, A. A., & Weisburd, D. L. (2013). Editor’s introduction: Advancing program evaluation methods in Criminology and Criminal Justice. <em>Evaluation Review</em>, <em>37</em>(3-4), 163-169. <a href="#fnref16" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn17" class="footnote-item">
            <p>Braga, A. A., & Weisburd, D. L. (2013). Editors’ Introduction: Advancing program evaluation methods in criminology and criminal justice. <em>Evaluation Review</em>, <em>37</em>(3-4), 163-169.; Lum, C., Yang, S. (2005). Why do evaluation researchers in crime and justice choose non-experimental methods? J<em>ournal of Experimental Criminology</em>, <em>1</em>, 191-213.; Nagin, D. S., & Weisburd, D. (2013). Evidence and public policy: The example of evaluation research in policing. <em>Criminology and Public Policy</em>, <em>12</em>(4), 651-679.; Weisburd, D. (2003). Ethical practice and evaluation of interventions in crime and justice: The moral imperative for randomized trials. <em>Evaluation Review</em>, <em>27</em>(3), 336-354.; Weisburd, D., C. Lum, and A. Petrosino. (2001). Does research design affect study outcomes in criminal justice?. <em>Annals of the American Academy of Political and Social Science</em>, <em>578</em>, 50–70.; Weisburd, D., Perosino, A., & Fronius, T. (2014). Randomized experiments in criminology and criminal justice. In David Weisburd and Gerben Bruinsma (Eds.) <em>Encyclopedia of criminology and criminal justice</em>, p. 4283-4291. New York: Springer Verlaag. <a href="#fnref17" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn18" class="footnote-item">
            <p>Campbell, D. T., & Stanley, J. C. (2015). <em>Experimental and quasi-experimental designs for research</em>. Boston, MA: Houghton Mifflin Company. <a href="#fnref18" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn19" class="footnote-item">
            <p>Escamilla, J., Reichert, J., Hillhouse, M., & Hawken, A. (2018). BetaGov supports practitioners and evaluators in conducting randomized control trials to test criminal justice programs. <em>Translational Criminology</em>, 15, 29-31. <a href="#fnref19" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn20" class="footnote-item">
            <p>Weisburd, D., Lum, C. M., & Petrosino, A. (2001). Does research design affect study outcomes in criminal justice?. <em>The Annals of the American Academy of Political and Social Science</em>, 578(1), 50-70. <a href="#fnref20" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn21" class="footnote-item">
            <p>American Evaluation Association. (2018). <em>American Evaluation Association guiding principles for evaluators</em>. Retrieved from <a href="http://bit.ly/2XiK2ng;">http://bit.ly/2XiK2ng;</a><br>
                Konrad, E. L. (2000). Commentary: Alleviating the fears of the anxious administrator. <em>American Journal of Evaluation</em>, <em>21</em>, 264–268.;<br>
                See 45 CFR 46 and U.S. Department of Health and Human Services, Office for Human Research Protections <a href="http://bit.ly/2Xg0Qex">http://bit.ly/2Xg0Qex</a> <a href="#fnref21" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn22" class="footnote-item">
            <p>Escamilla, J., Reichert, J., Hillhouse, M., & Hawken, A. (2018). BetaGov supports practitioners and evaluators in conducting randomized control trials to test criminal justice programs. <em>Translational Criminology</em>, 15, 29-31. <a href="#fnref22" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn23" class="footnote-item">
            <p>Eck, J. (2003). Police problems: The complexity of problem theory, research and evaluation. <em>Crime Prevention Studies</em>, <em>15</em>, 79-114. <a href="#fnref23" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn24" class="footnote-item">
            <p>Centers for Disease Control and Prevention. (n.d.) <em>Introduction to program evaluation for public health programs: A self-study guide</em>. Retrieved from <a href="http://bit.ly/2Xg0Z1z">http://bit.ly/2Xg0Z1z</a> <a href="#fnref24" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn25" class="footnote-item">
            <p>Rossi, P. H., Lipsey, M. W., & Freeman, H. E. (2004). <em>Evaluation: A systematic approach</em> (7th ed.). Thousand Oaks, CA: Sage Publications, Inc. <a href="#fnref25" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn26" class="footnote-item">
            <p>Solomon, M. A., & Shortell, S. M. (1981). Designing health policy research for utilization. <em>Health Policy Quarterly</em>, <em>1</em>(3), 216-237. <a href="#fnref26" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn27" class="footnote-item">
            <p>Centers for Disease Control and Prevention. (1999). <em>Framework for program evaluation in public health</em>. MMWR 48 (No. RR-11). <a href="#fnref27" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn28" class="footnote-item">
            <p>Gleicher, L. (2018). <em>Reducing substance use disorders and related offending: A continuum of evidence-informed practices in the criminal justice system</em>. Chicago, IL: Illinois Criminal Justice Information Authority. <a href="#fnref28" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn29" class="footnote-item">
            <p>Cross, A. B., Mulvey, E. P., Schubert, C. A., Griffin, P. A., Filone, S., Winckworth-Prejsnar, K., ... & Heilbrun, K. (2014). An agenda for advancing research on crisis intervention teams for mental health emergencies. <em>Psychiatric Services</em>, <em>65</em>(4), 530-536.;<br>
                Roesch, R. (1978). Does adult diversion work? The failure of research in criminal justice. <em>Crime & Delinquency</em>, <em>24</em>(1), 72-80. <a href="#fnref29" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn30" class="footnote-item">
            <p>Eck, J. (2003). Police problems: The complexity of problem theory, research and evaluation. <em>Crime Prevention Studies</em>, <em>15</em>, 79-114. <a href="#fnref30" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn31" class="footnote-item">
            <p>Corporation for National and Community Service. (n.d.) How to develop the right research questions for program evaluation. <a href="#fnref31" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn32" class="footnote-item">
            <p>Escamilla, J., Reichert, J., Hillhouse, M., & Hawken, A. (2018). BetaGov supports practitioners and evaluators in conducting randomized control trials to test criminal justice programs. <em>Translational Criminology</em>, 15, 29-31. <a href="#fnref32" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn33" class="footnote-item">
            <p>Escamilla, J., Reichert, J., Hillhouse, M., & Hawken, A. (2018). BetaGov supports practitioners and evaluators in conducting randomized control trials to test criminal justice programs. <em>Translational Criminology</em>, 15, 29-31. <a href="#fnref33" class="footnote-backref">↩︎</a></p>
        </li>
        <li id="fn34" class="footnote-item">
            <p>Short, J. F., Jr., Zahn, M. A., & Farrington, D. P. (2000). Experimental research in criminal justice settings: Is there a role for scholarly societies? <em>Crime & Delinquency</em>, <em>46</em>(3), 295-298. <a href="#fnref34" class="footnote-backref">↩︎</a></p>
        </li>
    </ol>
</section>